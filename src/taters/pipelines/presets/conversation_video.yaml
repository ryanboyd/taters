# conversation_video.yaml

meta:
  id: conversation_video                 # short, stable identifier (file stem by default)
  title: Conversation video → transcripts + features
  summary: >
    Extract audio, diarize, compute Whisper embeddings, then unify
    transcripts and get a whole boatload of text-based features/measures.
  use_cases:
    - Video recordings of conversations
    - Interview studies
    - Focus groups
    - etc.
  inputs:
    file type: video files (e.g., mkv, mp4, etc.)
#  outputs:
#    - path: ./transcripts/<stem>/<stem>.csv
#      desc: Diarized transcripts with start_time,end_time,speaker,text
#    - path: ./features/whisper-embeddings.csv
#      desc: Speaker-mean Whisper encoder embeddings
  requirements:
    cpu: true
    gpu/cuda: optional
    ffmpeg: true
    extras:
      - diarization
      - cuda
      - readability
  variables:
    device:
      default: auto
      desc: Which device you would like to use for pytorch-heavy stuff (cpu|cuda|auto)
    overwrite_existing:
      default: false
      desc: Do not overwrite outputs unless true
    whisper_model:
      default: base
      desc: Faster-Whisper model size
    num_speakers:
      default: null
      desc: For diarization - how many speakers would you like to try to cluster the data into?
    dictionaries_path:
      default: "./dictionaries/liwc"
      desc: > 
        A folder or list full of LIWC-formatted dictionary files 
        (.dicx, .dic, .csv) that you want to apply to the transcripts.
    archetypes_dict_path:
      default: "./dictionaries/archetypes"
      desc: >
        A folder or list of .CSV archetype dictionaries that you want 
        to apply to the transcripts.
    # Output roots (override if you want)
    transcripts_dir:
      default: "./transcripts/"
      desc: The base directory where you would like your individual transcripts to be saved.
    features_dir:
      default: "./features/"
      desc: The base directory where you would like your feature files to be saved.
  tags: [audio, video, diarization, embeddings]
  version: 2
  authors: ["Ryan L. Boyd"]
  notes: |
    Safe to re-run; steps short-circuit if outputs exist. See docs for tuning.
  cli_example: >
    python -m taters.pipelines.run_pipeline --root_dir video-data --file_type video 
    --preset conversation_video --workers 8 --var device=cuda

# Begin specification of pipeline

vars:
  device: "auto"
  overwrite_existing: false

  # audio stuff
  whisper_model: "base"
  num_speakers: null

  # Paths to different types of dictionary resources (folders)
  dictionaries_path: "dictionaries/liwc"
  archetypes_dict_path: "dictionaries/archetypes"

  # Output roots (override if you want)
  transcripts_dir: "transcripts"
  features_dir: "features"

steps:
# 1) Video → WAV
- scope: item
  call: potato.audio.convert_to_wav
  save_as: wav
  with:
    input_path: "{{input}}"
    # You can add sample_rate/bit_depth/channels if you want to standardize
    overwrite_existing: "{{var:overwrite_existing}}"

# 2) Diarize (writes transcript CSV/SRT/TXT)
- scope: item
  call: potato.audio.diarize_with_thirdparty
  with:
    audio_path: "{{wav}}"
    whisper_model: "{{var:whisper_model}}"
    device: "{{var:device}}"            # auto/cuda/cpu supported by wrapper
    batch_size: 0
    use_custom: true
    keep_temp: false
    num_speakers: "{{var:num_speakers}}"
  save_as: diar
  require: [audio_path]

# 3) Per-speaker WAVs from the diarization CSV
- scope: item
  call: potato.audio.split_wav_by_speaker
  save_as: speaker_wav
  with:
    source_wav: "{{wav}}"
    transcript_csv_path: "{{pick:diar.raw_files.csv}}"
    time_unit: "ms"
    overwrite_existing: "{{var:overwrite_existing}}"

# 4) Whisper embeddings (segment-level, from transcript)
- scope: item
  call: potato.audio.extract_whisper_embeddings
  save_as: whisper_feats
  with:
    source_wav: "{{wav}}"
    transcript_csv: "{{pick:diar.raw_files.csv}}"
    time_unit: "ms"
    model_name: "{{var:whisper_model}}"
    device: "{{var:device}}"
    overwrite_existing: "{{var:overwrite_existing}}"

# 5) GATHER whisper embeddings into a single CSV
#    Aggregate within each file by 'speaker' → mean of numeric cols.
- scope: global
  call: potato.helpers.feature_gather
  save_as: whisper_gathered
  with:
    root_dir: "{{var:features_dir}}/whisper-embeddings"
    pattern: "*.csv"
    aggregate: true
    group_by: ["speaker"]
    per_file: true                  # include 'source' as a grouping key
    stats: ["mean"]
    overwrite_existing: "{{var:overwrite_existing}}"
    out_csv: "features/whisper-embeddings_aggregated.csv"                   # default → ./features/whisper-embeddings.csv

# 6) GATHER transcripts (CSV) into a single unified CSV (no aggregation)
- scope: global
  call: potato.helpers.feature_gather
  save_as: transcripts_all
  with:
    root_dir: "{{var:transcripts_dir}}"
    pattern: "*.csv"
    recursive: true
    aggregate: false
    add_source_path: true
    overwrite_existing: "{{var:overwrite_existing}}"
    out_csv: "./all_transcripts.csv"   # unified transcript table

# 7) DICTIONARY CODING on the unified transcript CSV
#    Group by (source, speaker) to create per-speaker-per-file text.
- scope: global
  call: potato.text.analyze_with_dictionaries
  save_as: dict_features
  with:
    csv_path: "{{transcripts_all}}"      # feed the unified CSV
    out_features_csv: "features/dictionary.csv"
    text_cols: ["text"]
    id_cols: ["source","speaker"]        # carry these through
    group_by: ["source","speaker"]       # aggregate utterances → per speaker
    mode: "concat"
    dict_paths: ["{{var:dictionaries_path}}"]
    delimiter: ","
    encoding: "utf-8-sig"
    overwrite_existing: "{{var:overwrite_existing}}"
    relative_freq: true

# 8) READABILITY on the unified transcript CSV
#     Group by (source, speaker) so each speaker-per-file becomes one text blob.
- scope: global
  call: potato.text.analyze_readability
  save_as: readability_features
  with:
    csv_path: "{{transcripts_all}}"          # the unified transcript CSV from step 6
    out_features_csv: "features/readability.csv"
    text_cols: ["text"]
    id_cols: ["source","speaker"]            # carry these through
    group_by: ["source","speaker"]           # aggregate utterances → per speaker
    mode: "concat"                           # join rows within each (source,speaker)
    delimiter: ","
    encoding: "utf-8-sig"
    overwrite_existing: "{{var:overwrite_existing}}"


# 9) LEXICAL RICHNESS on the unified transcript CSV
#     Group by (source, speaker) so each speaker-per-file becomes one text blob.
- scope: global
  call: potato.text.analyze_lexical_richness
  save_as: lexrich_features
  with:
    csv_path: "{{transcripts_all}}"          # the unified transcript CSV from step 6
    out_features_csv: "features/lexical-richness.csv"
    text_cols: ["text"]
    id_cols: ["source","speaker"]            # carry these through
    group_by: ["source","speaker"]           # aggregate utterances → per speaker
    mode: "concat"                           # join rows within each (source,speaker)
    delimiter: ","
    encoding: "utf-8-sig"
    overwrite_existing: "{{var:overwrite_existing}}"

    # (Optional) metric hyperparameters — uncomment to customize
    # msttr_window: 100
    # mattr_window: 100
    # mtld_threshold: 0.72
    # hdd_draws: 42
    # vocd_ntokens: 50
    # vocd_within_sample: 100
    # vocd_iterations: 3
    # vocd_seed: 42


# 10) ARCHETYPES on the unified transcript CSV
#    Same grouping → comparable per-speaker rows.
- scope: global
  call: potato.text.analyze_with_archetypes
  save_as: archetype_features
  with:
    csv_path: "{{transcripts_all}}"
    out_features_csv: "features/archetypes.csv"
    text_cols: ["text"]
    id_cols: ["source","speaker"]
    group_by: ["source","speaker"]
    archetype_csvs: ["{{var:archetypes_dict_path}}"]
    model_name: "sentence-transformers/all-roberta-large-v1"
    mean_center_vectors: true
    rounding: 4
    overwrite_existing: "{{var:overwrite_existing}}"

# 11) SENTENCE EMBEDDINGS (row-level, no grouping, from unified CSV)
- scope: global
  call: potato.text.extract_sentence_embeddings
  save_as: sent_embeds
  with:
    csv_path: "{{transcripts_all}}"
    text_cols: ["text"]
    id_cols: ["source","speaker"]
    mode: "concat"
    model_name: "sentence-transformers/all-roberta-large-v1"
    normalize_l2: true
    pass_through_cols: ["source","speaker"]
    overwrite_existing: "{{var:overwrite_existing}}"

# 12) Aggregate SENTENCE EMBEDDINGS by (source, speaker) → mean
- scope: global
  call: potato.helpers.feature_gather
  save_as: sent_embeds_agg
  with:
    root_dir: "{{sent_embeds}}"
    aggregate: true
    group_by: ["source", "speaker"]   # ← these become leading columns in the output
    per_file: false                   # we’re already providing "source" explicitly
    stats: ["mean"]
    add_source_path: false
    exclude_cols: ["start_time","end_time","text"]
    overwrite_existing: false
    out_csv: "features/sentence-embeddings_aggregated.csv"

